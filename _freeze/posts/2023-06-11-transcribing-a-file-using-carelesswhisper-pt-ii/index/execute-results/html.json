{
  "hash": "a87ae2cf8d3f93a3bc3577e6c0352c53",
  "result": {
    "markdown": "---\ntitle: \"Transcribing A File Using carelesswhisper Pt II\"\nsubtitle: |\n  \ndate: 2023-06-11\ncategories: []\neditor_options:\n  chunk_output_type: console\nbibliography: refs.bib\n---\n\n\n## TL;DR \n\nYou can really improve the quality of the transcription you get from `{whisper}` if you use a bigger, language specific model.\n\n## Previously On ...\n\nIn my last post, I showed how you can use `{carelesswhisper}`, and the `{av}` and `{tuneR}` packages to quickly chunk up an audio file, and do audio transcription on each chunk, without having a large WAV file sitting on disk, or reading the large amound of WAV data into memory.\n\n## Improving Results\n\nI did hypothesize that using a bigger model would improve things, and *coolbutuseless* also commented on Mastodon that bigger, or more language specific models rapidly improve the quality of the audio transcription.\nSo, I thought I would try my hand at it and see what we can do.\n\n## targets\n\nSome of the models take a **long** time to run, so this document is actually built as it's own `{targets}` workflow, using the guidelines from the supplement of the the `{targets}` package manual [@targets_book].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\ntar_unscript()\n```\n:::\n\n::: {.cell tar_globals='true' tar_interactive='true'}\n::: {.cell tar_globals='true' tar_interactive='true' tar_name='functions' tar_script='_targets.R'}\n\n```{.targets .cell-code}\nsplit_time = function(audio_length)\n{\n  n_piece = ceiling(audio_length / 30) + 2\n  out_time = vector(\"list\", n_piece)\n  start_time = 0\n  for (i_piece in seq_len(n_piece)) {\n    if (start_time > audio_length) {\n      break()\n    }\n    out_time[[i_piece]] = c(start = start_time,\n                                         end = min(c(start_time + 30, audio_length)))\n    start_time = start_time + 28\n  }\n  \n  not_null = purrr::map_lgl(out_time, \\(x){!is.null(x)})\n  out_time[not_null]\n}\n\n# rescale the values in the wav object\nrescale_wav = function(wav_object)\n{\n  wav_values = wav_object@left\n  wav_range = max(abs(wav_values))\n  wav_sign = sign(wav_values)\n  wav_fraction = abs(wav_values) / wav_range\n  wav_convert = wav_sign * wav_fraction\n  wav_convert\n}\n\n# do the actual conversion and transcription\ninner_convert_transcribe = function(time_index, audio_file, whisper_model)\n{\n  out_wav = av::av_audio_convert(audio_file, output = \"tmp.wav\", channels = 1,\n                                 sample_rate = 16000, start_time = time_index[\"start\"],\n                                 total_time = time_index[\"end\"] - time_index[\"start\"],\n                                 verbose = FALSE)\n  in_audio = tuneR::readWave(\"tmp.wav\")\n  scaled_audio = rescale_wav(in_audio)\n  transcribed = carelesswhisper::whisper(whisper_model, scaled_audio)\n  transcribed\n}\n```\n:::::: {.cell tar_globals='true' tar_interactive='true' tar_name='functions' tar_script='_targets.R'}\n::: {.cell-output .cell-output-stderr}\n```\nRun code and assign objects to the environment.\n```\n:::\n:::::: {.cell tar_globals='true' tar_interactive='true' tar_name='functions' tar_script='_targets.R'}\n\n:::\n:::\n\n::: {.cell tar_interactive='true' tar_globals='true'}\n::: {.cell tar_interactive='true' tar_globals='true' tar_name='test-target' tar_script='_targets.R'}\n\n```{.targets .cell-code}\ntar_target(test_output, message(\"Hi!\"))\n```\n:::::: {.cell tar_interactive='true' tar_globals='true' tar_name='test-target' tar_script='_targets.R'}\n::: {.cell-output .cell-output-stderr}\n```\nRun code and assign objects to the environment.\n```\n:::\n:::::: {.cell tar_interactive='true' tar_globals='true' tar_name='test-target' tar_script='_targets.R'}\n::: {.cell-output .cell-output-stdout}\n```\n<tar_stem> \n  name: test_output \n  command:\n    message(\"Hi!\") \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    targets\n    stats\n    graphics\n    grDevices\n    datasets\n    utils\n    methods\n    base \n  library:\n    NULL\n```\n:::\n:::\n:::\n\n\n## Original Functions\n\nHere are the original functions we used.\n\n\n## Updated To Use Different Models\n\nOur wrapper function, we are going to modify slightly to be able to use a different model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert an actual file\nconvert_transcribe = function(audio_file, model_file)\n{\n  t_start = Sys.time()\n  audio_info = av::av_media_info(audio_file)\n  audio_length = audio_info$duration\n  \n  time_splits = split_time(audio_length)\n  whisper_model = carelesswhisper::whisper_init(model_file)\n  transcribed_data = purrr::map(time_splits,\n                                inner_convert_transcribe,\n                                audio_file,\n                                whisper_model)\n  file.remove(\"tmp.wav\")\n  t_end = Sys.time()\n  list(transcription = transcribed_data,\n       elapsed = as.numeric(difftime(t_end, t_start, units = \"secs\")))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naudio_file = \"20s vs 40s Vacation [DFM4Ey5oyhI].m4a\"\nmodel_files = dir(\".\", pattern = \"bin$\")\n\nmodel_results = purrr::map(model_files, \\(x){\n  message(x)\n  convert_transcribe(audio_file, x)\n  }\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}